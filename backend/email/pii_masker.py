import re
import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification
from pathlib import Path
from functools import lru_cache

LABEL_LIST = [
    "O",
    "B-ì´ë¦„", "I-ì´ë¦„",
    "B-ì£¼ë¯¼ë²ˆí˜¸", "I-ì£¼ë¯¼ë²ˆí˜¸",
    "B-ì „í™”ë²ˆí˜¸", "I-ì „í™”ë²ˆí˜¸",
    "B-ì´ë©”ì¼", "I-ì´ë©”ì¼",
    "B-ì¹´ë“œë²ˆí˜¸", "I-ì¹´ë“œë²ˆí˜¸",
    "B-ì£¼ì†Œ", "I-ì£¼ì†Œ"
]
id2label = {i: label for i, label in enumerate(LABEL_LIST)}


BASE_DIR = Path(__file__).resolve().parent          # .../backend/email
BACKEND_DIR = BASE_DIR.parent                       # .../backend
MODEL_DIR = BACKEND_DIR / "ner_model"               #

MODEL_PATH = MODEL_DIR.resolve()

tokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH), local_files_only=True)
model = AutoModelForTokenClassification.from_pretrained(str(MODEL_PATH), local_files_only=True)
model.eval()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ” NER ì˜ˆì¸¡
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ner_predict(text):
    tokens = list(text)
    tokenized = tokenizer(tokens, is_split_into_words=True, return_tensors="pt", truncation=True)
    with torch.no_grad():
        output = model(**tokenized)
    preds = output.logits.argmax(dim=-1).squeeze().tolist()
    word_ids = tokenized.word_ids()
    merged = []
    prev_id = None
    for idx, wid in enumerate(word_ids):
        if wid is None or wid == prev_id:
            continue
        merged.append((tokens[wid], id2label[preds[idx]]))
        prev_id = wid
    return merged

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§© ì—”í„°í‹° ë³‘í•©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def merge_entities(tagged):
    result = []
    tag = None
    buffer = ""
    for char, label in tagged:
        if label.startswith("B-"):
            if tag:
                result.append((buffer, tag))
            tag = label[2:]
            buffer = char
        elif label.startswith("I-") and tag == label[2:]:
            buffer += char
        else:
            if tag:
                result.append((buffer, tag))
            tag, buffer = None, ""
    if tag:
        result.append((buffer, tag))
    return result

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”’ ê°œë³„ ì—”í„°í‹° ë§ˆìŠ¤í‚¹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def mask_entity(text, label):
    if label == "ì´ë¦„":
        return text[0] + "**" if len(text) >= 2 else text + "*"
    elif label == "ì£¼ë¯¼ë²ˆí˜¸":
        return re.sub(r"\d{6}-\d{7}", lambda m: m.group(0)[:6] + "-*******", text)
    elif label == "ì „í™”ë²ˆí˜¸":
        return re.sub(r"\d{2,3}-\d{3,4}-\d{4}", lambda m: m.group(0)[:3] + "-****-" + m.group(0)[-4:], text)
    elif label == "ì´ë©”ì¼":
        local, _, domain = text.partition("@")
        return local[0] + "***@" + domain
    elif label == "ì¹´ë“œë²ˆí˜¸":
        return re.sub(r"\d{4}-\d{4}-\d{4}-\d{4}", lambda m: f"{m.group(0)[:4]}-****-****-{m.group(0)[-4:]}", text)
    elif label == "ì£¼ì†Œ":
        return re.sub(r"([ê°€-í£]{2,}(ì‹œ|ë„)\s?[ê°€-í£]{1,}(êµ¬|êµ°|ì‹œ)).*", lambda m: m.group(1) + " ****", text)
    else:
        return text

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ” ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ ë§ˆìŠ¤í‚¹ ë³´ì™„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def regex_based_mask(text):
    # ì£¼ë¯¼ë²ˆí˜¸
    text = re.sub(r"\d{6}-\d{7}", lambda m: m.group(0)[:6] + "-*******", text)

    # ì „í™”ë²ˆí˜¸
    phone_patterns = [
        r"\b01[016789]-?\d{3,4}-?\d{4}\b",
        r"\b\d{2,3}-\d{3,4}-\d{4}\b",
        r"\(\d{2,3}\)\s?\d{3,4}-\d{4}\b"
    ]
    for p in phone_patterns:
        text = re.sub(p, lambda m: m.group(0)[:3] + "-****-" + m.group(0)[-4:], text)

    # ì´ë©”ì¼
    text = re.sub(r"\b([a-zA-Z0-9._%+-]+)@([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})\b",
                  lambda m: m.group(1)[0] + "***@" + m.group(2), text)

    # ì¹´ë“œë²ˆí˜¸
    text = re.sub(r"\b(\d{4})-(\d{4})-(\d{4})-(\d{4})\b",
                  lambda m: f"{m.group(1)}-****-****-{m.group(4)}", text)

    # ì§€ë²ˆ ì£¼ì†Œ
    text = re.sub(r"([ê°€-í£]{2,}(ì‹œ|ë„)\s?[ê°€-í£]{1,}(êµ¬|êµ°|ì‹œ)).*",
                  lambda m: m.group(1) + " ****", text)

    # ë„ë¡œëª… ì£¼ì†Œ 
    text = re.sub(
        r"([ê°€-í£]{2,}(ì‹œ|ë„)\s*[ê°€-í£0-9\-]+(ë¡œ|ê¸¸|ëŒ€ë¡œ)\s*\d{1,4}(ë²ˆ?ê¸¸)?(\s*\d{0,4})?)",
        lambda m: m.group(1).split()[0] + " ****",
        text
    )

    # ê³„ì¢Œë²ˆí˜¸: 3-2-6 í˜•ì‹ ë˜ëŠ” ìˆ«ìë§Œ (10~14ìë¦¬)
    text = re.sub(
        r"\b\d{2,4}-\d{2,4}-\d{2,6}\b",
        lambda m: m.group(0)[:4] + "-****-" + m.group(0)[-4:],
        text
    )
    text = re.sub(
        r"\b\d{10,14}\b",
        lambda m: m.group(0)[:4] + "****" + m.group(0)[-4:],
        text
    )

    return text

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”’ ì „ì²´ ë§ˆìŠ¤í‚¹ ì ìš©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def mask_text(text):
    tagged = ner_predict(text)
    entities = merge_entities(tagged)
    masked = text
    offset = 0
    for word, label in entities:
        if not word.strip():
            continue
        start = masked.find(word, offset)
        if start == -1:
            continue
        end = start + len(word)
        masked_word = mask_entity(word, label)
        masked = masked[:start] + masked_word + masked[end:]
        offset = start + len(masked_word)
    return regex_based_mask(masked)


def contains_sensitive_keywords(text, keywords):
    return any(k in text for k in keywords)
